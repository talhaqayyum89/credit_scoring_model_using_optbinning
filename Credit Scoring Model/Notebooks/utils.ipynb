{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datetime as dt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string \n",
    "import copy\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Scorecard Modelling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from optbinning import Scorecard\n",
    "from optbinning import BinningProcess\n",
    "\n",
    "# Extras:\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from typing import Tuple\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_zero_values_table(df):\n",
    "  \n",
    "    \"\"\"\n",
    "    function for data audit, returns the count and percentage of missing and o values in each column\n",
    "     %% parameters \n",
    "\n",
    "    df: [dataframe] \n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    zero_val = (df == 0.00).astype(int).sum(axis=0)\n",
    "        \n",
    "    zero_val_percent = (df == 0.00).astype(int).sum(axis=0) / len(df)\n",
    "        \n",
    "    mis_val = df.isnull().sum()\n",
    "    \n",
    "    mis_val_percent = round(df.isnull().sum() / len(df),1)\n",
    "    \n",
    "    mz_table = pd.concat([zero_val,zero_val_percent, mis_val, mis_val_percent], axis=1)\n",
    "    \n",
    "    mz_table = mz_table.rename(\n",
    "                                columns = {0 : 'No of 0s', 1 : '% of 0s', 2: 'No of Missing Values', 3 : '% of Missing Values'})\n",
    "    \n",
    "    mz_table['Rows'] = len(df)\n",
    "    mz_table['No of Unique'] = df.nunique()\n",
    "    \n",
    "    mz_table['Data Type'] = df.dtypes\n",
    "    \n",
    "    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n",
    "        \"There are \" + str(mz_table.shape[0]) +\n",
    "            \" columns that have missing values.\")\n",
    "    \n",
    "#         mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)\n",
    "    return mz_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from optbinning.scorecard import plot_auc_roc, plot_cap, plot_ks\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def plot_all_metrics(y_train, y_test, train_pred, test_pred, model, X_train, X_test):\n",
    "    \"\"\"\n",
    "    Generate and display all relevant plots: AUC-ROC, CAP, KS, Precision-Recall, and Confusion Matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_train (array-like): True labels for the training set.\n",
    "    - y_test (array-like): True labels for the test set.\n",
    "    - train_pred (array-like): Predictions for the training set.\n",
    "    - test_pred (array-like): Predictions for the test set.\n",
    "    - model: Trained model (needed for confusion matrix plot).\n",
    "    - X_train (array-like): Training features (for confusion matrix plot).\n",
    "    - X_test (array-like): Test features (for confusion matrix plot).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a figure with subplots (5 rows and 2 columns for better organization)\n",
    "    fig, axs = plt.subplots(5, 2, figsize=(15, 25))\n",
    "    \n",
    "    # AUC-ROC for training set (top-left)\n",
    "    plt.sca(axs[0, 0])  # Set the current axis to top-left\n",
    "    plot_auc_roc(y_train, train_pred)\n",
    "    axs[0, 0].set_title('AUC-ROC for Training Set')\n",
    "\n",
    "    # AUC-ROC for test set (top-right)\n",
    "    plt.sca(axs[0, 1])  # Set the current axis to top-right\n",
    "    plot_auc_roc(y_test, test_pred)\n",
    "    axs[0, 1].set_title('AUC-ROC for Test Set')\n",
    "\n",
    "    # CAP for training set (middle-left)\n",
    "    plt.sca(axs[1, 0])  # Set the current axis to middle-left\n",
    "    plot_cap(y_train, train_pred)\n",
    "    axs[1, 0].set_title('CAP for Training Set')\n",
    "\n",
    "    # CAP for test set (middle-right)\n",
    "    plt.sca(axs[1, 1])  # Set the current axis to middle-right\n",
    "    plot_cap(y_test, test_pred)\n",
    "    axs[1, 1].set_title('CAP for Test Set')\n",
    "\n",
    "    # KS for training set (third row, left)\n",
    "    plt.sca(axs[2, 0])  # Set the current axis to bottom-left\n",
    "    plot_ks(y_train, train_pred)\n",
    "    axs[2, 0].set_title('KS for Training Set')\n",
    "\n",
    "    # KS for test set (third row, right)\n",
    "    plt.sca(axs[2, 1])  # Set the current axis to bottom-right\n",
    "    plot_ks(y_test, test_pred)\n",
    "    axs[2, 1].set_title('KS for Test Set')\n",
    "\n",
    "    # Precision-Recall Curve for training set (fourth row, left)\n",
    "    plt.sca(axs[3, 0])\n",
    "    precision_train, recall_train, _ = precision_recall_curve(y_train, train_pred)\n",
    "    pr_auc_train = auc(recall_train, precision_train)\n",
    "    axs[3, 0].plot(recall_train, precision_train, label=f\"PR AUC = {pr_auc_train:.2f}\")\n",
    "    axs[3, 0].set_title('Precision-Recall Curve for Training Set')\n",
    "    axs[3, 0].set_xlabel('Recall')\n",
    "    axs[3, 0].set_ylabel('Precision')\n",
    "    axs[3, 0].legend()\n",
    "\n",
    "    # Precision-Recall Curve for test set (fourth row, right)\n",
    "    plt.sca(axs[3, 1])\n",
    "    precision_test, recall_test, _ = precision_recall_curve(y_test, test_pred)\n",
    "    pr_auc_test = auc(recall_test, precision_test)\n",
    "    axs[3, 1].plot(recall_test, precision_test, label=f\"PR AUC = {pr_auc_test:.2f}\")\n",
    "    axs[3, 1].set_title('Precision-Recall Curve for Test Set')\n",
    "    axs[3, 1].set_xlabel('Recall')\n",
    "    axs[3, 1].set_ylabel('Precision')\n",
    "    axs[3, 1].legend()\n",
    "\n",
    "    # Confusion Matrix for test set (fifth row, left)\n",
    "    plt.sca(axs[4, 0])\n",
    "    # Step 1: Predict probabilities for the test set using the scorecard object\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Step 2: Binarize the predictions (using a threshold of 0.5 for example)\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # Step 3: Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ConfusionMatrixDisplay(cm, display_labels=['Non-Default', 'Default']).plot(cmap='Blues', ax=axs[4, 1])\n",
    "    axs[4, 1].set_title('Confusion Matrix for Test Set')\n",
    "\n",
    "    # Confusion Matrix for training set (fifth row, right)\n",
    "    plt.sca(axs[4, 0])\n",
    "    y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train = (y_pred_proba_train >= threshold).astype(int)\n",
    "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    ConfusionMatrixDisplay(cm_train, display_labels=['Non-Default', 'Default']).plot(cmap='Blues', ax=axs[4, 0])\n",
    "    axs[4, 0].set_title('Confusion Matrix for Training Set')\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import brier_score_loss, calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import category_encoders as ce\n",
    "\n",
    "def calibration_analysis(X_train, X_test, y_train, y_test, scorecard_model):\n",
    "    \"\"\"\n",
    "    Perform calibration analysis using Platt scaling and isotonic regression.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (DataFrame): Training feature set.\n",
    "    - X_test (DataFrame): Test feature set.\n",
    "    - y_train (Series): Training target variable.\n",
    "    - y_test (Series): Test target variable.\n",
    "    - scorecard_model: A fitted scorecard model to compare non-calibrated results.\n",
    "\n",
    "    Returns:\n",
    "    - Plots calibration curves and prints Brier scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "    # Logistic regression estimator\n",
    "    estimator = LogisticRegression(solver=\"lbfgs\", class_weight='balanced')\n",
    "\n",
    "    # Preprocessing pipeline for numerical and categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', ce.TargetEncoder(cols=categorical_features), categorical_features)\n",
    "        ])\n",
    "\n",
    "    # Logistic regression pipeline\n",
    "    logreg_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', estimator)\n",
    "    ])\n",
    "\n",
    "    # Non-calibrated predictions from your scorecard model\n",
    "    y_train_pred_proba = scorecard_model.predict_proba(X_train)[:, -1]\n",
    "    y_test_pred_proba = scorecard_model.predict_proba(X_test)[:, -1]\n",
    "\n",
    "    # Brier score for non-calibrated predictions from scorecard\n",
    "    non_calibrated_brier = brier_score_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "    # Calibrate using Platt scaling\n",
    "    platt_model = CalibratedClassifierCV(base_estimator=logreg_pipeline, method='sigmoid')\n",
    "    platt_model.fit(X_train, y_train)\n",
    "    platt_pred_calibrated = platt_model.predict_proba(X_test)[:, 1]\n",
    "    platt_train_pred_proba = platt_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "    # Calibrate using Isotonic regression\n",
    "    iso_model = CalibratedClassifierCV(base_estimator=logreg_pipeline, method='isotonic')\n",
    "    iso_model.fit(X_train, y_train)\n",
    "    iso_pred_calibrated = iso_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate Brier scores for calibrated predictions\n",
    "    platt_brier = brier_score_loss(y_test, platt_pred_calibrated)\n",
    "    iso_brier = brier_score_loss(y_test, iso_pred_calibrated)\n",
    "\n",
    "    # Print Brier scores\n",
    "    print(f'Non-Calibrated Brier Score (Scorecard): {non_calibrated_brier:.4f}')\n",
    "    print(f'Platt Scaling Brier Score: {platt_brier:.4f}')\n",
    "    print(f'Isotonic Regression Brier Score: {iso_brier:.4f}')\n",
    "\n",
    "    # Calibration curve using the correct test labels\n",
    "    prob_true, prob_pred_non_calibrated = calibration_curve(y_test, y_test_pred_proba, n_bins=10)\n",
    "    prob_true_calibrated_platt, prob_pred_calibrated_platt = calibration_curve(y_test, platt_pred_calibrated, n_bins=10)\n",
    "    prob_true_calibrated_isotonic, prob_pred_calibrated_isotonic = calibration_curve(y_test, iso_pred_calibrated, n_bins=10)\n",
    "\n",
    "    # Plotting calibration curves with Brier score annotations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(prob_pred_non_calibrated, prob_true, marker='o', label=f'Non-calibrated (Scorecard, Brier: {non_calibrated_brier:.3f})', color='blue')\n",
    "    plt.plot(prob_pred_calibrated_platt, prob_true_calibrated_platt, marker='o', label=f'Platt Calibrated (Brier: {platt_brier:.3f})', color='green')\n",
    "    plt.plot(prob_pred_calibrated_isotonic, prob_true_calibrated_isotonic, marker='o', label=f'Isotonic Calibrated (Brier: {iso_brier:.3f})', color='red')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='black')  # Perfect calibration line\n",
    "\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('True Probability')\n",
    "    plt.title('Calibration Plot: Scorecard vs Platt & Isotonic Calibration')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_levels_decriptions = {\n",
    "    1: \"Very Poor\",\n",
    "    2: \"Poor\",\n",
    "    3: \"Below Average\",\n",
    "    4: \"Average\",\n",
    "    5: \"Above Average\",\n",
    "    6: \"Good\",\n",
    "    7: \"Very Good\",\n",
    "    8: \"Excellent\",\n",
    "    9: \"Exceptional\",\n",
    "}\n",
    "\n",
    "def get_credit_levels(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"credit_score\",\n",
    "    left_bound = -np.inf,\n",
    "    level_1 = 350,\n",
    "    level_2 = 400,\n",
    "    level_3 = 450,\n",
    "    level_4 = 500,\n",
    "    level_5 = 550,\n",
    "    level_6 = 600,\n",
    "    level_7 = 650,\n",
    "    level_8 = 700,\n",
    "    right_bound = np.inf\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Explain the credit levels and description for all FICO credit scores.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the credit score\n",
    "        target_col (str): Column containing FICO credit score.\n",
    "        left_bound (int): Lowest possible FICO credit score.\n",
    "        level_1 (int): Value where the credit scores are equal or below will be categorize as level 1.\n",
    "        level_2 (int): Value where the credit scores are equal or below will be categorize as level 2.\n",
    "        level_3 (int): Value where the credit scores are equal or below will be categorize as level 3.\n",
    "        level_4 (int): Value where the credit scores are equal or below will be categorize as level 4.\n",
    "        right_bound (int): Lowest possible FICO credit score.\n",
    "\n",
    "    Returns:\n",
    "        float: The dataframe containing the credit levels and descriptions for all credit scores.\n",
    "    \"\"\"\n",
    "    conditions = [\n",
    "        (df[target_col] > left_bound) & (df[target_col] <= level_1),\n",
    "        (df[target_col] > level_1) & (df[target_col] <= level_2),\n",
    "        (df[target_col] > level_2) & (df[target_col] <= level_3),\n",
    "        (df[target_col] > level_3) & (df[target_col] <= level_4),\n",
    "        (df[target_col] > level_4) & (df[target_col] <= level_5),\n",
    "        (df[target_col] > level_5) & (df[target_col] <= level_6),\n",
    "        (df[target_col] > level_6) & (df[target_col] <= level_7),\n",
    "        (df[target_col] > level_7) & (df[target_col] <= level_8),\n",
    "        (df[target_col] > level_8) & (df[target_col] <= right_bound),\n",
    "    ]\n",
    "\n",
    "    level_choices = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    lower_bound_choices = [left_bound, level_1, level_2, level_3, level_4, level_5, level_6, level_7, level_8]\n",
    "    upper_bound_choices = [level_1, level_2, level_3, level_4, level_5, level_6, level_7, level_8, right_bound]\n",
    "    df[\"credit_level\"] = np.select(conditions, level_choices)\n",
    "    df[\"credit_lower_bound\"] = np.select(conditions, lower_bound_choices)\n",
    "    df[\"credit_upper_bound\"] = np.select(conditions, upper_bound_choices)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate ROC AUC (Area Under the Receiver Operating Characteristic Curve).\n",
    "    \n",
    "    Args:\n",
    "        y_true (Union[list, np.array]): True labels.\n",
    "        y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "    Returns:\n",
    "        float: ROC AUC score.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "def pr_auc(y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate PR AUC (Area Under the Precision Recall Curve).\n",
    "    \n",
    "    Args:\n",
    "        y_true (Union[list, np.array]): True labels.\n",
    "        y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "    Returns:\n",
    "        float: PR AUC score.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "def gini(y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Gini coefficient.\n",
    "\n",
    "    Args:\n",
    "        y_true (Union[list, np.array]): True labels.\n",
    "        y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "    Returns:\n",
    "        float: Gini coefficient.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return 2 * roc_auc - 1\n",
    "\n",
    "def ks(y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Kolmogorov-Smirnov (KS) statistic.\n",
    "\n",
    "    Args:\n",
    "        y_true (Union[list, np.array]): True labels.\n",
    "        y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "    Returns:\n",
    "        float: KS statistic.\n",
    "    \"\"\"\n",
    "    y_pred_proba_not_default = y_pred_proba[y_true == 0]\n",
    "    y_pred_proba_default = y_pred_proba[y_true == 1]\n",
    "    ks_stat, _ = stats.ks_2samp(y_pred_proba_not_default, y_pred_proba_default)\n",
    "    return ks_stat\n",
    "\n",
    "def plot_calibration_curve(y_true: np.array, y_pred_proba: np.array, model_name: str, figsize: Tuple[int, int], n_bins=10) -> plt.Axes:\n",
    "    \"\"\"\n",
    "    Plot calibration curve.\n",
    "\n",
    "    Args:\n",
    "        y_pred_proba (np.array): Predicted probabilities for the positive class (default).\n",
    "        y_true (np.array): True binary labels (0 for not default, 1 for default).\n",
    "        model_name (str): Name of the model for labeling the plot.\n",
    "        figsize (Tuple[int, int]): size of the plot.\n",
    "        n_bins (int): Number of bins to use for calibration curve.\n",
    "    Return:\n",
    "        plt.Axes: Matplotlib axis object.\n",
    "    \"\"\"\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_pred_proba, n_bins=n_bins)\n",
    "    \n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly calibrated\")\n",
    "    ax.plot(prob_pred, prob_true, marker=\"o\", label=model_name)\n",
    "    \n",
    "    ax.set_xlabel(\"Mean predicted probability\")\n",
    "    ax.set_ylabel(\"Fraction of positives\")\n",
    "    ax.set_title(\"Calibration plot\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def print_side_by_side(dict1: dict, dict2: dict) -> None:\n",
    "    \"\"\"\n",
    "    Prints the content of two dictionaries side by side.\n",
    "\n",
    "    Args:\n",
    "        dict1 (dict): The first dictionary to be printed.\n",
    "        dict2 (dict): The second dictionary to be printed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Calculate the maximum length of keys in both dictionaries\n",
    "    max_key_len = max(max(len(key) for key in dict1), max(len(key) for key in dict2))\n",
    "    \n",
    "    # Define the format string for printing, adjusted for floating point numbers\n",
    "    format_str = \"{:<{key_len}}: {:<10} | {:<10}\"\n",
    "    \n",
    "    # Print header\n",
    "    print(format_str.format(\"Metric\", \"Train\", \"Test\", key_len=max_key_len))\n",
    "    \n",
    "    # Print separator\n",
    "    print(\"-\" * (max_key_len + 24))  # Adjusted separator length\n",
    "    \n",
    "    # Print key-value pairs side by side, rounding floats if necessary\n",
    "    for key in dict1:\n",
    "        val1 = dict1[key]\n",
    "        val2 = dict2[key]\n",
    "        \n",
    "        # Check if the values are float, if so, round to 2 decimal places\n",
    "        if isinstance(val1, float):\n",
    "            val1 = round(val1, 2)\n",
    "        if isinstance(val2, float):\n",
    "            val2 = round(val2, 2)\n",
    "        \n",
    "        # Print the formatted output\n",
    "        print(format_str.format(key, val1, val2, key_len=max_key_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to calculate WOE and IV for numerical features\n",
    "def woe_num(data, feature, gbflag):\n",
    "    dt = pd.crosstab(index=data[feature], columns=data[gbflag])\n",
    "    dt['Freq'] = dt.sum(axis=1)\n",
    "    dt['Percentage'] = round((dt['Freq']/dt['Freq'].sum() *100),1)\n",
    "    dt['% Good'] = round((dt['Good']/dt['Good'].sum() *100), 1)\n",
    "    dt['% Bad'] = round((dt['Bad']/dt['Bad'].sum() *100), 1)\n",
    "    dt['Bad Rate'] = round((dt['Bad']/dt['Freq']) *100, 1) \n",
    "    dt['GoodBaddOdds'] = round(dt['Good']/dt['Bad'], 2)\n",
    "    dt['WOE'] = np.log(dt['% Bad']/dt['% Good'])\n",
    "    dt['Class IV'] = (dt['% Bad']- dt['% Good']) * dt['WOE']\n",
    "    dt['Variable IV'] = ((dt['% Bad']-dt['% Good']) * dt['WOE']).sum() #IV for the variable\n",
    "#     dt['IV'] = ((dt['% Good']- dt['% Bad']) * dt['WOE']).sum()\n",
    "    dt = dt.sort_values(['WOE'])\n",
    "    \n",
    "    return dt\n",
    "# Function to calculate WOE and IV for categorical variables\n",
    "# Function to calculate WOE and IV for categorical variables\n",
    "def woe_cat(data, feature, gbflag):\n",
    "    dt = pd.crosstab(index=data[feature], columns=data[gbflag])\n",
    "    dt['Freq'] = dt.sum(axis=1)\n",
    "    dt['Proptn'] = dt['Freq']/dt['Freq'].sum()\n",
    "    dt['% Good'] = dt['Good']/dt['Good'].sum() \n",
    "    dt['% Bad'] = dt['Bad']/dt['Bad'].sum()\n",
    "    dt['Bad Rate'] = dt['Bad']/dt['Freq'] \n",
    "    dt['GoodBaddOdds'] = round(dt['Good']/dt['Bad'], 2)\n",
    "    dt['WOE'] = np.log(dt['% Bad']/dt['% Good'])\n",
    "    dt['Class IV'] = (dt['% Bad']- dt['% Good']) * dt['WOE']\n",
    "    dt['Variable IV'] = ((dt['% Bad']-dt['% Good']) * dt['WOE']).sum() #IV for the variable\n",
    "    dt = dt.sort_values(['WOE'])\n",
    "    return dt, dt['Variable IV'].iloc[0]\n",
    "\n",
    "\n",
    "# A function to get all variables WOE and IV\n",
    "def woe_iv(df, variablie_list, GBFlag):   \n",
    "    output = {}\n",
    "    for variable in variablie_list:\n",
    "        try:\n",
    "            var = woe_cat(df, variable, GBFlag)\n",
    "            \n",
    "            for i in range(len(var.index.values)):\n",
    "                output.setdefault('Variables',[]).append(variable)\n",
    "                output.setdefault('Categories',[]).append(var.index.values[i])\n",
    "                output.setdefault('WOE',[]).append(var['WOE'].values[i])\n",
    "                output.setdefault('IV',[]).append(var['IV'].values[i])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame(output)\n",
    "# woe_iv(data_universe_fai, data_universe_fai_cat.columns, 'GBFlag')\n",
    "# A function that automates and return CA report\n",
    "def Export_CA_Report(df, variablie_list, GBFlag, fileName='CA_Report'):\n",
    "    \n",
    "    # This inner function estimates WoE and IV which represent CA for all the provided variables\n",
    "    def CA_Report(df, variablie_list, GBFlag):\n",
    "        # Create a dictionary to track CA iteratively\n",
    "        output = {}\n",
    "        for variable in variablie_list:\n",
    "            # use the function for estimating WoE for categorical variables to \n",
    "            var = woe_cat(df, variable, GBFlag)\n",
    "            # append to dictionary\n",
    "            output[variable]=var        \n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "   # Instatiate above function to CA report for all provided variables\n",
    "    ca_result = CA_Report(df, variablie_list, GBFlag)\n",
    "    # save CA report to Excel\n",
    "    with pd.ExcelWriter(fileName +'.xlsx', engine=\"openpyxl\") as writer:\n",
    "        for variable_name, data in ca_result.items():\n",
    "            data.to_excel(writer, sheet_name=variable_name)\n",
    "    \n",
    "    \n",
    "def fill_binned_nan_values(df, variablie_list, GBFlag):\n",
    "    for variable in variablie_list:\n",
    "        if df[variable].isna().sum() > 0:\n",
    "            # Run CA(WoE & IV) on variable\n",
    "            run_ca = woe_num(df, variable, GBFlag)\n",
    "            # get index value(bin class) of category with least woe\n",
    "            bin_class = run_ca[run_ca['WOE']==max(run_ca['WOE'])].index.values[0]\n",
    "            df[variable] = df[variable].fillna(bin_class)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_woe(df_with_selected_variables, variables_woe_iv):\n",
    "    '''\n",
    "    This function replaces each category in the selected columns with\n",
    "    their woe\n",
    "    \n",
    "    df_with_selected_variables :: Dataframe containing grouped/binned selected\n",
    "    variables\n",
    "    \n",
    "    grpd_variables_woe_iv :: Dataframe containing WoE and IV of grouped/binned\n",
    "    variables\n",
    "    \n",
    "    Note:: Make sure woe_iv function has returned it's result before running\n",
    "    this function. result in this case - grpd_variables_woe_iv\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for variable in df_with_selected_variables.columns:\n",
    "        for category in df_with_selected_variables[variable].unique():\n",
    "            \n",
    "            try:\n",
    "                ### Get WoE from the dataframe running the WoE_IV function\n",
    "                woe =variables_woe_iv[\n",
    "                    (variables_woe_iv['Variables']==variable) & \n",
    "                    (variables_woe_iv['Categories']==category)\n",
    "                ]['WOE'].values[0]\n",
    "\n",
    "                ### Replace each category with their respective woe\n",
    "                df_with_selected_variables[variable] = df_with_selected_variables[variable].replace(category, woe)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    return df_with_selected_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woe_iv(df, features, target):   \n",
    "    output = {}\n",
    "    for variable in features:\n",
    "        # Adjusted to unpack the tuple into var (DataFrame) and iv (scalar value)\n",
    "        var, iv = woe_cat(df, variable, target)\n",
    "        \n",
    "        for i in range(len(var.index)):\n",
    "            # Now assuming 'var' correctly references the DataFrame part of the tuple\n",
    "            output.setdefault('Variables', []).append(variable)\n",
    "            output.setdefault('Categories', []).append(var.index[i])  # Accessing index directly\n",
    "            output.setdefault('WOE', []).append(var['WOE'].iloc[i])  # Using .iloc[] for accessing values\n",
    "            # Instead of appending var['IV'].iloc[i], append the iv variable for each category\n",
    "            output.setdefault('IV', []).append(iv)  # Assuming iv is a scalar value representing the IV for the entire variable\n",
    "\n",
    "    # Convert the dictionary to DataFrame\n",
    "    return pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "def compute_gini(y_test, y_train, test_preds, train_preds):\n",
    "    # instantiate roc auc score from sci-kit learn metrics and compute AUC\n",
    "\n",
    "    Test_AUC = roc_auc_score(y_test, test_preds)\n",
    "    Train_AUC = roc_auc_score(y_train, train_preds)\n",
    "    # Compute Gini from AUC\n",
    "    Test_Gini = (2*Test_AUC) - 1\n",
    "    Train_Gini = (2*Train_AUC) - 1\n",
    "    return Test_Gini, Train_Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_by_woe(df_WoE, feature_name, rotation_of_x_axis_labels=0, xlabel_fontsize=12, ylabel_fontsize=12):\n",
    "    x = np.array(df_WoE.index).astype(str)\n",
    "    y = df_WoE['WOE']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x, y, marker='o', linestyle='--', color='k')\n",
    "    plt.xlabel('Categories', fontsize=xlabel_fontsize)\n",
    "    plt.ylabel('Weight of Evidence', fontsize=ylabel_fontsize)\n",
    "    plt.title(f'Weight of Evidence by {feature_name}', fontsize=14)\n",
    "    plt.xticks(rotation=rotation_of_x_axis_labels)\n",
    "    plt.grid(True)  # Add gridlines for better visualization\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_by_woe_plotly(df_WoE, feature_name):\n",
    "    x = np.array(df_WoE.index).astype(str)\n",
    "    y = df_WoE['WOE']\n",
    "    \n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=x, y=y, mode='lines+markers', name='WoE', line=dict(color='black')))\n",
    "    \n",
    "    # Update layout with titles and labels\n",
    "    fig.update_layout(\n",
    "        title=f'Weight of Evidence by {feature_name}',\n",
    "        xaxis_title='Categories',\n",
    "        yaxis_title='Weight of Evidence',\n",
    "        xaxis=dict(tickmode='array', tickvals=list(range(len(x))), ticktext=x),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def calculate_woe_and_iv_for_all_features_plotly(df_train, df_test, features, target):\n",
    "    iv_summary = {'Feature': [], 'Train IV': [], 'Test IV': []}\n",
    "\n",
    "    for feature in features:\n",
    "        # Calculate WoE and IV for training and testing datasets\n",
    "        woe_df_train, iv_train = woe_cat(df_train, feature, target)\n",
    "        woe_df_test, iv_test = woe_cat(df_test, feature, target)\n",
    "\n",
    "        # Create Plotly figures for WoE of training and testing datasets\n",
    "        fig_train = plot_by_woe_plotly(woe_df_train, f'Train {feature}')\n",
    "        fig_test = plot_by_woe_plotly(woe_df_test, f'Test {feature}')\n",
    "\n",
    "        # Setup for side-by-side subplots\n",
    "        fig = make_subplots(rows=1, cols=2, subplot_titles=(f'Train {feature}', f'Test {feature}'))\n",
    "        \n",
    "        # Add traces from the individual plots to the combined subplot\n",
    "        for trace in fig_train['data']:\n",
    "            fig.add_trace(trace, row=1, col=1)\n",
    "        \n",
    "        for trace in fig_test['data']:\n",
    "            fig.add_trace(trace, row=1, col=2)\n",
    "        \n",
    "        # Display the subplot\n",
    "        fig.show()\n",
    "\n",
    "        # Append IV values to the summary\n",
    "        iv_summary['Feature'].append(feature)\n",
    "        iv_summary['Train IV'].append(iv_train)\n",
    "        iv_summary['Test IV'].append(iv_test)\n",
    "\n",
    "    return woe_results, pd.DataFrame(iv_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_vif(X):\n",
    "    \"\"\"\n",
    "    Calculate VIF for each variable in the given DataFrame.\n",
    "    \"\"\"\n",
    "    X_const = sm.add_constant(X)\n",
    "    vif_df = pd.DataFrame({\n",
    "        'Variable': X_const.columns,\n",
    "        'VIF': [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]\n",
    "    })\n",
    "    return vif_df\n",
    "\n",
    "def remove_high_vif_variables(X, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Remove variables with VIF greater than the specified threshold and return\n",
    "    the reduced DataFrame along with the DataFrame of VIF values for the remaining variables.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        vif_df = calculate_vif(X)\n",
    "        vif_df = vif_df[vif_df['Variable'] != 'const']  # Exclude the constant term\n",
    "        high_vif = vif_df[vif_df['VIF'] > threshold]\n",
    "        \n",
    "        if high_vif.empty:\n",
    "            break  # No variables with VIF > threshold\n",
    "        \n",
    "        # Remove the variable with the highest VIF\n",
    "        highest_vif_var = high_vif.sort_values('VIF', ascending=False).iloc[0]['Variable']\n",
    "        X = X.drop(columns=[highest_vif_var])\n",
    "    \n",
    "    # Calculate the final VIF DataFrame for the remaining variables\n",
    "    final_vif_df = calculate_vif(X)\n",
    "    return X, final_vif_df.drop(index=0)  # Drop the row corresponding to the 'const' term\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distribution(df, feature):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df, x=feature, hue='IsDefault', palette='coolwarm')\n",
    "    plt.title(f'Distribution by {feature}')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_credit_score_distributions(data, features):\n",
    "    \"\"\"\n",
    "    Plot distributions of credit score by a list of categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data.\n",
    "    - features: List of strings representing the categorical features to plot.\n",
    "    \"\"\"\n",
    "    num_features = len(features)\n",
    "    # Calculate the number of rows/columns needed for the subplots\n",
    "    nrows = (num_features + 1) // 2\n",
    "    ncols = 2 if num_features > 1 else 1\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    f, axes = plt.subplots(nrows, ncols, figsize=(18, 6 * nrows))\n",
    "    \n",
    "    # Flatten axes array for easy indexing if there's more than one subplot\n",
    "    if nrows * ncols > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot distributions of credit score by specified features\n",
    "    for i, feature in enumerate(features):\n",
    "        sns.boxplot(x=feature, y='credit_score', data=data, ax=axes[i])\n",
    "        axes[i].set_title(f'Credit Score by {feature.capitalize()}')\n",
    "        axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(data):\n",
    "    '''\n",
    "    This function estimates the statistical significance of categorical \n",
    "    variables with respect to predicting the target variable\n",
    "    \n",
    "    parameters ::\n",
    "    - data :: DataFrame Object\n",
    "    \n",
    "    output ::\n",
    "    - Dataframe result of variables in order of significance\n",
    "    '''\n",
    "    # define an empty dictionary to store chi-test results\n",
    "    chi2_check = {}\n",
    "    \n",
    "    # select only categorical variables\n",
    "#     training_data_cat = data.select_dtypes(include = ['object','category'])\n",
    "    \n",
    "    # iteratively pick columns and calculate chi statistic with the target variable\n",
    "    for column in data.columns.tolist()[:-1]:\n",
    "    \n",
    "        chi, p, dof, ex = chi2_contingency(pd.crosstab(data['IsDefault'], data[column]))\n",
    "        chi2_check.setdefault('Features',[]).append(column)\n",
    "        chi2_check.setdefault('p-values',[]).append(round(p, 3))\n",
    "\n",
    "    # convert the dictionary to a DF\n",
    "    chi2_result = pd.DataFrame(data = chi2_check).sort_values('p-values', ascending=True, ignore_index=True)\n",
    "    \n",
    "    # return result\n",
    "    return chi2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_months_loan_duration(value):\n",
    "    \"\"\"\n",
    "    Assigns a value to its corresponding bin for months_loan_duration.\n",
    "    \n",
    "    Parameters:\n",
    "    - value: The raw input value for months_loan_duration.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the binned category.\n",
    "    \"\"\"\n",
    "    bins = [(4.0, 12.0), (12.0, 18.0), (18.0, 24.0), (24.0, 72.0)]\n",
    "    bin_labels = ['(3.999, 12.0]', '(12.0, 18.0]', '(18.0, 24.0]', '(24.0, 72.0]']\n",
    "\n",
    "    for bin_range, label in zip(bins, bin_labels):\n",
    "        if bin_range[0] < value <= bin_range[1]:\n",
    "            return label\n",
    "    return '(3.999, 12.0]'  # Return 'Unknown' or another appropriate value if the value doesn't fit in any bin\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_amount(value):\n",
    "    \"\"\"\n",
    "    Assigns an amount value to its corresponding bin.\n",
    "    \n",
    "    Parameters:\n",
    "    - value: The raw input value for the amount.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the binned category.\n",
    "    \"\"\"\n",
    "    bins = [(250.0, 1365.5), (1365.5, 2319.5), (2319.5, 3972.25), (3972.25, 18424.0)]\n",
    "    bin_labels = ['(249.999, 1365.5]', '(1365.5, 2319.5]', '(2319.5, 3972.25]', '(3972.25, 18424.0]']\n",
    "\n",
    "    for bin_range, label in zip(bins, bin_labels):\n",
    "        if bin_range[0] < value <= bin_range[1]:\n",
    "            return label\n",
    "    return '(249.999, 1365.5]' # Return 'Unknown' if the value doesn't fit in any bin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_savings_balance(value):\n",
    "    \"\"\"\n",
    "    Assigns a savings balance value to its corresponding bin.\n",
    "    \n",
    "    Parameters:\n",
    "    - value: The raw input value for the savings balance.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the binned category.\n",
    "    \"\"\"\n",
    "    bins = [(-0.001, 10.75), (10.75, 49.5), (49.5, 91.25), (91.25, 19972.0)]\n",
    "    bin_labels = ['(-0.001, 10.75]', '(10.75, 49.5]', '(49.5, 91.25]', '(91.25, 19972.0]']\n",
    "\n",
    "    for bin_range, label in zip(bins, bin_labels):\n",
    "        if bin_range[0] < value <= bin_range[1]:\n",
    "            return label\n",
    "    return '(-0.001, 10.75)'  # Return 'Unknown' if the value doesn't fit in any bin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_age(value):\n",
    "    \"\"\"\n",
    "    Assigns an age value to its corresponding bin.\n",
    "    \n",
    "    Parameters:\n",
    "    - value: The raw input value for age.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the binned category.\n",
    "    \"\"\"\n",
    "    bins = [(19.0, 27.0), (27.0, 33.0), (33.0, 42.0), (42.0, 75.0)]\n",
    "    bin_labels = ['(18.999, 27.0]', '(27.0, 33.0]', '(33.0, 42.0]', '(42.0, 75.0]']\n",
    "\n",
    "    for bin_range, label in zip(bins, bin_labels):\n",
    "        if bin_range[0] < value <= bin_range[1]:\n",
    "            return label\n",
    "    return '(18.999, 27.0]'  # Return 'Unknown' if the value doesn't fit in any bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years(employment_length):\n",
    "    \"\"\"\n",
    "    Extracts the numeric part from the employment length input, which might be a string or float.\n",
    "    \n",
    "    Parameters:\n",
    "    - employment_length: Input describing the employment length, which could be a string like '2 years', a float, or 'Unknown'.\n",
    "    \n",
    "    Returns:\n",
    "    - The numeric part (number of years) as an integer, 'Unknown' if not applicable, or the input directly if it's already numeric.\n",
    "    \"\"\"\n",
    "    if pd.isnull(employment_length) or employment_length == 'Unknown':\n",
    "        return 'Unknown'\n",
    "    elif isinstance(employment_length, (int, float)):\n",
    "        return employment_length  # Return directly if already a number\n",
    "    else:\n",
    "        try:\n",
    "            # Assuming the format could be '[number] years' if it's a string\n",
    "            return int(employment_length.split()[0])\n",
    "        except ValueError:\n",
    "            return 'Unknown'\n",
    "\n",
    "def bin_employment_length(employment_length):\n",
    "    \"\"\"\n",
    "    Assigns an employment length value to its corresponding bin, handling both string and numeric inputs.\n",
    "    \n",
    "    Parameters:\n",
    "    - employment_length: The raw input value for employment length, which could be a string or numeric.\n",
    "    \n",
    "    Returns:\n",
    "    - A string representing the binned category.\n",
    "    \"\"\"\n",
    "    years = extract_years(employment_length)\n",
    "    \n",
    "    if years == 'Unknown':\n",
    "        return 'Unknown'\n",
    "    elif years == 0:\n",
    "        return '0 years'\n",
    "    elif 0 < years < 5:\n",
    "        return '< 5 years'\n",
    "    elif 5 <= years < 10:\n",
    "        return '5-10 years'\n",
    "    elif 10 <= years < 15:\n",
    "        return '10-15 years'\n",
    "    elif years >= 15:\n",
    "        return '> 15 years'\n",
    "    else:\n",
    "        return 'Unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from itertools import cycle\n",
    "\n",
    "def calculate_store_and_plot_bad_rates(df, grouping_columns, outcome_col='IsDefault'):\n",
    "    if outcome_col not in df.columns:\n",
    "        raise ValueError(f\"Error: '{outcome_col}' column does not exist.\")\n",
    "    \n",
    "    if not grouping_columns:\n",
    "        raise ValueError(\"No grouping columns provided.\")\n",
    "    \n",
    "    # Initialize subplot figure with dynamic configuration based on the provided grouping columns\n",
    "    cols = len(grouping_columns)\n",
    "    color_cycle = cycle(px.colors.qualitative.Plotly)\n",
    "    fig = make_subplots(rows=1, cols=cols, subplot_titles=[f'Bad Rate(%) by {col}' for col in grouping_columns])\n",
    "    \n",
    "    for idx, group_col in enumerate(grouping_columns):\n",
    "        if group_col not in df.columns:\n",
    "            raise ValueError(f\"Error: '{group_col}' column does not exist.\")\n",
    "        \n",
    "        temp_df = df[[group_col, outcome_col]].copy()\n",
    "        temp_df[group_col] = temp_df[group_col].astype(str)\n",
    "        \n",
    "        crosstab = pd.crosstab(temp_df[group_col], temp_df[outcome_col])\n",
    "        \n",
    "        if not {'Bad', 'Good'}.issubset(crosstab.columns):\n",
    "            print(f\"Warning: 'Good' and 'Bad' labels not found in '{outcome_col}' for group '{group_col}'.\")\n",
    "            continue\n",
    "        \n",
    "        total = crosstab['Bad'] + crosstab['Good']\n",
    "        bad_rate = (crosstab['Bad'] / total) * 100\n",
    "        crosstab_sorted = bad_rate.sort_values(ascending=False)\n",
    "        \n",
    "        fig.add_trace(go.Bar(x=crosstab_sorted.index, y=crosstab_sorted.values, name=group_col, marker_color=next(color_cycle)), row=1, col=idx + 1)\n",
    "    \n",
    "    fig.update_layout(height=600, width=400 * cols, title_text=\"Comparison of Bad Rates Across Categories\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_woe_and_iv_for_all_features(df_train, df_test, features, target):\n",
    "    # Function definitions for `woe_cat` and `plot_by_woe` should be included or imported here\n",
    "\n",
    "    woe_results = {}\n",
    "    iv_summary = {'Feature': [], 'Train IV': [], 'Test IV': []}\n",
    "\n",
    "    for feature in features:\n",
    "        # Calculate WoE and IV for the training and testing datasets\n",
    "        woe_df_train, iv_train = woe_cat(df_train, feature, target)\n",
    "        woe_df_test, iv_test = woe_cat(df_test, feature, target)\n",
    "\n",
    "        # Plot WoE for training and testing datasets\n",
    "        plot_by_woe(woe_df_train, feature)\n",
    "        plot_by_woe(woe_df_test, feature)\n",
    "\n",
    "        # Combine Train and Test DataFrames with a spacer for visualization\n",
    "        spacer_df = pd.DataFrame({(' ', ' ', f'Spacer_{feature}'): [None] * len(woe_df_train) for _ in range(1, 4)})\n",
    "        woe_combined = pd.concat([woe_df_train, spacer_df, woe_df_test], axis=1)\n",
    "        woe_results[feature] = woe_combined\n",
    "\n",
    "        # Append IV values to the summary\n",
    "        iv_summary['Feature'].append(feature)\n",
    "        iv_summary['Train IV'].append(iv_train)\n",
    "        iv_summary['Test IV'].append(iv_test)\n",
    "\n",
    "    return woe_results, pd.DataFrame(iv_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
