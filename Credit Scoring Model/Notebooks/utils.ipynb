{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datetime as dt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string \n",
    "import copy\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Scorecard Modelling:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from optbinning import Scorecard\n",
    "from optbinning import BinningProcess\n",
    "\n",
    "# Extras:\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from typing import Tuple\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_zero_values_table(df):\n",
    "  \n",
    "    \"\"\"\n",
    "    function for data audit, returns the count and percentage of missing and o values in each column\n",
    "     %% parameters \n",
    "\n",
    "    df: [dataframe] \n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    zero_val = (df == 0.00).astype(int).sum(axis=0)\n",
    "        \n",
    "    zero_val_percent = (df == 0.00).astype(int).sum(axis=0) / len(df)\n",
    "        \n",
    "    mis_val = df.isnull().sum()\n",
    "    \n",
    "    mis_val_percent = round(df.isnull().sum() / len(df),1)\n",
    "    \n",
    "    mz_table = pd.concat([zero_val,zero_val_percent, mis_val, mis_val_percent], axis=1)\n",
    "    \n",
    "    mz_table = mz_table.rename(\n",
    "                                columns = {0 : 'No of 0s', 1 : '% of 0s', 2: 'No of Missing Values', 3 : '% of Missing Values'})\n",
    "    \n",
    "    mz_table['Rows'] = len(df)\n",
    "    mz_table['No of Unique'] = df.nunique()\n",
    "    \n",
    "    mz_table['Data Type'] = df.dtypes\n",
    "    \n",
    "    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n",
    "        \"There are \" + str(mz_table.shape[0]) +\n",
    "            \" columns that have missing values.\")\n",
    "    \n",
    "#         mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx', freeze_panes=(1,0), index = False)\n",
    "    return mz_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from optbinning.scorecard import plot_auc_roc, plot_cap, plot_ks\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "def plot_all_metrics(y_train, y_test, train_pred, test_pred, model, X_train, X_test):\n",
    "    \"\"\"\n",
    "    Generate and display all relevant plots: AUC-ROC, CAP, KS, Precision-Recall, and Confusion Matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_train (array-like): True labels for the training set.\n",
    "    - y_test (array-like): True labels for the test set.\n",
    "    - train_pred (array-like): Predictions for the training set.\n",
    "    - test_pred (array-like): Predictions for the test set.\n",
    "    - model: Trained model (needed for confusion matrix plot).\n",
    "    - X_train (array-like): Training features (for confusion matrix plot).\n",
    "    - X_test (array-like): Test features (for confusion matrix plot).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a figure with subplots (5 rows and 2 columns for better organization)\n",
    "    fig, axs = plt.subplots(5, 2, figsize=(15, 25))\n",
    "    \n",
    "    # AUC-ROC for training set (top-left)\n",
    "    plt.sca(axs[0, 0])  # Set the current axis to top-left\n",
    "    plot_auc_roc(y_train, train_pred)\n",
    "    axs[0, 0].set_title('AUC-ROC for Training Set')\n",
    "\n",
    "    # AUC-ROC for test set (top-right)\n",
    "    plt.sca(axs[0, 1])  # Set the current axis to top-right\n",
    "    plot_auc_roc(y_test, test_pred)\n",
    "    axs[0, 1].set_title('AUC-ROC for Test Set')\n",
    "\n",
    "    # CAP for training set (middle-left)\n",
    "    plt.sca(axs[1, 0])  # Set the current axis to middle-left\n",
    "    plot_cap(y_train, train_pred)\n",
    "    axs[1, 0].set_title('CAP for Training Set')\n",
    "\n",
    "    # CAP for test set (middle-right)\n",
    "    plt.sca(axs[1, 1])  # Set the current axis to middle-right\n",
    "    plot_cap(y_test, test_pred)\n",
    "    axs[1, 1].set_title('CAP for Test Set')\n",
    "\n",
    "    # KS for training set (third row, left)\n",
    "    plt.sca(axs[2, 0])  # Set the current axis to bottom-left\n",
    "    plot_ks(y_train, train_pred)\n",
    "    axs[2, 0].set_title('KS for Training Set')\n",
    "\n",
    "    # KS for test set (third row, right)\n",
    "    plt.sca(axs[2, 1])  # Set the current axis to bottom-right\n",
    "    plot_ks(y_test, test_pred)\n",
    "    axs[2, 1].set_title('KS for Test Set')\n",
    "\n",
    "    # Precision-Recall Curve for training set (fourth row, left)\n",
    "    plt.sca(axs[3, 0])\n",
    "    precision_train, recall_train, _ = precision_recall_curve(y_train, train_pred)\n",
    "    pr_auc_train = auc(recall_train, precision_train)\n",
    "    axs[3, 0].plot(recall_train, precision_train, label=f\"PR AUC = {pr_auc_train:.2f}\")\n",
    "    axs[3, 0].set_title('Precision-Recall Curve for Training Set')\n",
    "    axs[3, 0].set_xlabel('Recall')\n",
    "    axs[3, 0].set_ylabel('Precision')\n",
    "    axs[3, 0].legend()\n",
    "\n",
    "    # Precision-Recall Curve for test set (fourth row, right)\n",
    "    plt.sca(axs[3, 1])\n",
    "    precision_test, recall_test, _ = precision_recall_curve(y_test, test_pred)\n",
    "    pr_auc_test = auc(recall_test, precision_test)\n",
    "    axs[3, 1].plot(recall_test, precision_test, label=f\"PR AUC = {pr_auc_test:.2f}\")\n",
    "    axs[3, 1].set_title('Precision-Recall Curve for Test Set')\n",
    "    axs[3, 1].set_xlabel('Recall')\n",
    "    axs[3, 1].set_ylabel('Precision')\n",
    "    axs[3, 1].legend()\n",
    "\n",
    "    # Confusion Matrix for test set (fifth row, left)\n",
    "    plt.sca(axs[4, 0])\n",
    "    # Step 1: Predict probabilities for the test set using the scorecard object\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Step 2: Binarize the predictions (using a threshold of 0.5 for example)\n",
    "    threshold = 0.5\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "\n",
    "    # Step 3: Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    ConfusionMatrixDisplay(cm, display_labels=['Non-Default', 'Default']).plot(cmap='Blues', ax=axs[4, 1])\n",
    "    axs[4, 1].set_title('Confusion Matrix for Test Set')\n",
    "\n",
    "    # Confusion Matrix for training set (fifth row, right)\n",
    "    plt.sca(axs[4, 0])\n",
    "    y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train = (y_pred_proba_train >= threshold).astype(int)\n",
    "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    ConfusionMatrixDisplay(cm_train, display_labels=['Non-Default', 'Default']).plot(cmap='Blues', ax=axs[4, 0])\n",
    "    axs[4, 0].set_title('Confusion Matrix for Training Set')\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# # from sklearn.metrics import brier_score_loss, calibration_curve\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import category_encoders as ce\n",
    "\n",
    "# def calibration_analysis(X_train, X_test, y_train, y_test, scorecard_model):\n",
    "#     \"\"\"\n",
    "#     Perform calibration analysis using Platt scaling and isotonic regression.\n",
    "\n",
    "#     Parameters:\n",
    "#     - X_train (DataFrame): Training feature set.\n",
    "#     - X_test (DataFrame): Test feature set.\n",
    "#     - y_train (Series): Training target variable.\n",
    "#     - y_test (Series): Test target variable.\n",
    "#     - scorecard_model: A fitted scorecard model to compare non-calibrated results.\n",
    "\n",
    "#     Returns:\n",
    "#     - Plots calibration curves and prints Brier scores.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Identify categorical and numerical features\n",
    "#     categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "#     numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "#     # Logistic regression estimator\n",
    "#     estimator = LogisticRegression(solver=\"lbfgs\", class_weight='balanced')\n",
    "\n",
    "#     # Preprocessing pipeline for numerical and categorical columns\n",
    "#     preprocessor = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('num', StandardScaler(), numerical_features),\n",
    "#             ('cat', ce.TargetEncoder(cols=categorical_features), categorical_features)\n",
    "#         ])\n",
    "\n",
    "#     # Logistic regression pipeline\n",
    "#     logreg_pipeline = Pipeline(steps=[\n",
    "#         ('preprocessor', preprocessor),\n",
    "#         ('classifier', estimator)\n",
    "#     ])\n",
    "\n",
    "#     # Non-calibrated predictions from your scorecard model\n",
    "#     y_train_pred_proba = scorecard_model.predict_proba(X_train)[:, -1]\n",
    "#     y_test_pred_proba = scorecard_model.predict_proba(X_test)[:, -1]\n",
    "\n",
    "#     # Brier score for non-calibrated predictions from scorecard\n",
    "#     non_calibrated_brier = brier_score_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "#     # Calibrate using Platt scaling\n",
    "#     platt_model = CalibratedClassifierCV(base_estimator=logreg_pipeline, method='sigmoid')\n",
    "#     platt_model.fit(X_train, y_train)\n",
    "#     platt_pred_calibrated = platt_model.predict_proba(X_test)[:, 1]\n",
    "#     platt_train_pred_proba = platt_model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "#     # Calibrate using Isotonic regression\n",
    "#     iso_model = CalibratedClassifierCV(base_estimator=logreg_pipeline, method='isotonic')\n",
    "#     iso_model.fit(X_train, y_train)\n",
    "#     iso_pred_calibrated = iso_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#     # Calculate Brier scores for calibrated predictions\n",
    "#     platt_brier = brier_score_loss(y_test, platt_pred_calibrated)\n",
    "#     iso_brier = brier_score_loss(y_test, iso_pred_calibrated)\n",
    "\n",
    "#     # Print Brier scores\n",
    "#     print(f'Non-Calibrated Brier Score (Scorecard): {non_calibrated_brier:.4f}')\n",
    "#     print(f'Platt Scaling Brier Score: {platt_brier:.4f}')\n",
    "#     print(f'Isotonic Regression Brier Score: {iso_brier:.4f}')\n",
    "\n",
    "#     # Calibration curve using the correct test labels\n",
    "#     prob_true, prob_pred_non_calibrated = calibration_curve(y_test, y_test_pred_proba, n_bins=10)\n",
    "#     prob_true_calibrated_platt, prob_pred_calibrated_platt = calibration_curve(y_test, platt_pred_calibrated, n_bins=10)\n",
    "#     prob_true_calibrated_isotonic, prob_pred_calibrated_isotonic = calibration_curve(y_test, iso_pred_calibrated, n_bins=10)\n",
    "\n",
    "#     # Plotting calibration curves with Brier score annotations\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(prob_pred_non_calibrated, prob_true, marker='o', label=f'Non-calibrated (Scorecard, Brier: {non_calibrated_brier:.3f})', color='blue')\n",
    "#     plt.plot(prob_pred_calibrated_platt, prob_true_calibrated_platt, marker='o', label=f'Platt Calibrated (Brier: {platt_brier:.3f})', color='green')\n",
    "#     plt.plot(prob_pred_calibrated_isotonic, prob_true_calibrated_isotonic, marker='o', label=f'Isotonic Calibrated (Brier: {iso_brier:.3f})', color='red')\n",
    "\n",
    "#     plt.plot([0, 1], [0, 1], linestyle='--', color='black')  # Perfect calibration line\n",
    "\n",
    "#     plt.xlabel('Predicted Probability')\n",
    "#     plt.ylabel('True Probability')\n",
    "#     plt.title('Calibration Plot: Scorecard vs Platt & Isotonic Calibration')\n",
    "#     plt.legend()\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "\n",
    "#     return platt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import brier_score_loss, roc_auc_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def gini_coefficient(y_true, y_scores):\n",
    "    \"\"\"Calculate the Gini coefficient.\"\"\"\n",
    "    # Compute AUC\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    # Gini = 2*AUC - 1\n",
    "    return 2 * auc - 1\n",
    "\n",
    "def cap(y_true, y_scores, n_bins=10):\n",
    "    \"\"\"Calculate Cumulative Accuracy Profile (CAP).\"\"\"\n",
    "    total_positive = y_true.sum()\n",
    "    if total_positive == 0:\n",
    "        return np.zeros(n_bins)  # Avoid division by zero if no positives\n",
    "\n",
    "    # Sort scores in descending order and get the indices\n",
    "    sorted_indices = np.argsort(y_scores)[::-1]\n",
    "    \n",
    "    # Use .iloc to ensure correct indexing\n",
    "    sorted_true = y_true.iloc[sorted_indices]  \n",
    "\n",
    "    # Cumulative sum of positives\n",
    "    cumulative_positive = np.cumsum(sorted_true)\n",
    "\n",
    "    # Calculate CAP\n",
    "    return cumulative_positive / total_positive\n",
    "\n",
    "def calibration_analysis(X_train, X_test, y_train, y_test, scorecard_model):\n",
    "    \"\"\"\n",
    "    Perform calibration analysis using Platt scaling and isotonic regression.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (DataFrame): Training feature set.\n",
    "    - X_test (DataFrame): Test feature set.\n",
    "    - y_train (Series): Training target variable.\n",
    "    - y_test (Series): Test target variable.\n",
    "    - scorecard_model: A fitted scorecard model to compare non-calibrated results.\n",
    "\n",
    "    Returns:\n",
    "    - Plots calibration curves and prints Brier scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "\n",
    "    # Preprocessing pipeline for numerical and categorical columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', ce.TargetEncoder(cols=categorical_features), categorical_features)\n",
    "        ])\n",
    "\n",
    "    # Logistic regression estimator\n",
    "    estimator = LogisticRegression(solver=\"lbfgs\", class_weight='balanced')\n",
    "\n",
    "    # Logistic regression pipeline\n",
    "    logreg_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', estimator)\n",
    "    ])\n",
    "\n",
    "    # Fit the logreg_pipeline on the training data\n",
    "    logreg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Non-calibrated predictions from your scorecard model\n",
    "    y_train_pred_proba = scorecard_model.predict_proba(X_train)[:, -1]\n",
    "    y_test_pred_proba = scorecard_model.predict_proba(X_test)[:, -1]\n",
    "\n",
    "    # Fit Platt model\n",
    "    platt_model = CalibratedClassifierCV(estimator=logreg_pipeline, method='sigmoid')\n",
    "    platt_model.fit(X_train, y_train)\n",
    "    platt_pred_proba = platt_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Fit Isotonic model\n",
    "    iso_model = CalibratedClassifierCV(estimator=logreg_pipeline, method='isotonic')\n",
    "    iso_model.fit(X_train, y_train)\n",
    "    iso_pred_proba = iso_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calculate Brier scores\n",
    "    non_calibrated_brier = brier_score_loss(y_test, y_test_pred_proba)\n",
    "    platt_brier = brier_score_loss(y_test, platt_pred_proba)\n",
    "    iso_brier = brier_score_loss(y_test, iso_pred_proba)\n",
    "\n",
    "    # Calibration curve using the correct test labels\n",
    "    prob_true, prob_pred_non_calibrated = calibration_curve(y_test, y_test_pred_proba, n_bins=10)\n",
    "    prob_true_calibrated_platt, prob_pred_calibrated_platt = calibration_curve(y_test, platt_pred_proba, n_bins=10)\n",
    "    prob_true_calibrated_isotonic, prob_pred_calibrated_isotonic = calibration_curve(y_test, iso_pred_proba, n_bins=10)\n",
    "\n",
    "    # Plotting calibration curves with Brier score annotations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(prob_pred_non_calibrated, prob_true, marker='o', label=f'Non-calibrated (Scorecard, Brier: {non_calibrated_brier:.3f})', color='blue')\n",
    "    plt.plot(prob_pred_calibrated_platt, prob_true_calibrated_platt, marker='o', label=f'Platt Calibrated (Brier: {platt_brier:.3f})', color='green')\n",
    "    plt.plot(prob_pred_calibrated_isotonic, prob_true_calibrated_isotonic, marker='o', label=f'Isotonic Calibrated (Brier: {iso_brier:.3f})', color='red')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='black')  # Perfect calibration line\n",
    "\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('True Probability')\n",
    "    plt.title('Calibration Plot: Scorecard vs Platt & Isotonic Calibration')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': ['Scorecard', 'Platt', 'Isotonic'],\n",
    "        'AUC-ROC Train': [\n",
    "            roc_auc_score(y_train, y_train_pred_proba),\n",
    "            roc_auc_score(y_train, platt_model.predict_proba(X_train)[:, 1]),\n",
    "            roc_auc_score(y_train, iso_model.predict_proba(X_train)[:, 1])\n",
    "        ],\n",
    "        'AUC-ROC Test': [\n",
    "            roc_auc_score(y_test, y_test_pred_proba),\n",
    "            roc_auc_score(y_test, platt_pred_proba),\n",
    "            roc_auc_score(y_test, iso_pred_proba)\n",
    "        ],\n",
    "        'Precision Train': [\n",
    "            precision_score(y_train, (y_train_pred_proba >= 0.5).astype(int)),\n",
    "            precision_score(y_train, (platt_model.predict_proba(X_train)[:, 1] >= 0.5).astype(int)),\n",
    "            precision_score(y_train, (iso_model.predict_proba(X_train)[:, 1] >= 0.5).astype(int))\n",
    "        ],\n",
    "        'Precision Test': [\n",
    "            precision_score(y_test, (y_test_pred_proba >= 0.5).astype(int)),\n",
    "            precision_score(y_test, (platt_pred_proba >= 0.5).astype(int)),\n",
    "            precision_score(y_test, (iso_pred_proba >= 0.5).astype(int))\n",
    "        ],\n",
    "        'Recall Train': [\n",
    "            recall_score(y_train, (y_train_pred_proba >= 0.5).astype(int)),\n",
    "            recall_score(y_train, (platt_model.predict_proba(X_train)[:, 1] >= 0.5).astype(int)),\n",
    "            recall_score(y_train, (iso_model.predict_proba(X_train)[:, 1] >= 0.5).astype(int))\n",
    "        ],\n",
    "        'Recall Test': [\n",
    "            recall_score(y_test, (y_test_pred_proba >= 0.5).astype(int)),\n",
    "            recall_score(y_test, (platt_pred_proba >= 0.5).astype(int)),\n",
    "            recall_score(y_test, (iso_pred_proba >= 0.5).astype(int))\n",
    "        ],\n",
    "        'Gini Train': [\n",
    "            gini_coefficient(y_train, y_train_pred_proba),\n",
    "            gini_coefficient(y_train, platt_model.predict_proba(X_train)[:, 1]),\n",
    "            gini_coefficient(y_train, iso_model.predict_proba(X_train)[:, 1])\n",
    "        ],\n",
    "        'Gini Test': [\n",
    "            gini_coefficient(y_test, y_test_pred_proba),\n",
    "            gini_coefficient(y_test, platt_pred_proba),\n",
    "            gini_coefficient(y_test, iso_pred_proba)\n",
    "        ],\n",
    "        # 'CAP Train': [\n",
    "        #     cap(y_train, y_train_pred_proba),\n",
    "        #     cap(y_train, platt_model.predict_proba(X_train)[:, 1]),\n",
    "        #     cap(y_train, iso_model.predict_proba(X_train)[:, 1])\n",
    "        # ],\n",
    "        # 'CAP Test': [\n",
    "        #     cap(y_test, y_test_pred_proba),\n",
    "        #     cap(y_test, platt_pred_proba),\n",
    "        #     cap(y_test, iso_pred_proba)\n",
    "        # ],\n",
    "    }\n",
    "\n",
    "    # Create DataFrame from metrics\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    display(metrics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_levels_decriptions = {\n",
    "    1: \"Very Poor\",\n",
    "    2: \"Poor\",\n",
    "    3: \"Below Average\",\n",
    "    4: \"Average\",\n",
    "    5: \"Above Average\",\n",
    "    6: \"Good\",\n",
    "    7: \"Very Good\",\n",
    "    8: \"Excellent\",\n",
    "    9: \"Exceptional\",\n",
    "}\n",
    "\n",
    "def get_credit_levels(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str = \"credit_score\",\n",
    "    left_bound = -np.inf,\n",
    "    level_1 = 350,\n",
    "    level_2 = 400,\n",
    "    level_3 = 450,\n",
    "    level_4 = 500,\n",
    "    level_5 = 550,\n",
    "    level_6 = 600,\n",
    "    level_7 = 650,\n",
    "    level_8 = 700,\n",
    "    right_bound = np.inf\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Explain the credit levels and description for all FICO credit scores.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the credit score\n",
    "        target_col (str): Column containing FICO credit score.\n",
    "        left_bound (int): Lowest possible FICO credit score.\n",
    "        level_1 (int): Value where the credit scores are equal or below will be categorize as level 1.\n",
    "        level_2 (int): Value where the credit scores are equal or below will be categorize as level 2.\n",
    "        level_3 (int): Value where the credit scores are equal or below will be categorize as level 3.\n",
    "        level_4 (int): Value where the credit scores are equal or below will be categorize as level 4.\n",
    "        right_bound (int): Lowest possible FICO credit score.\n",
    "\n",
    "    Returns:\n",
    "        float: The dataframe containing the credit levels and descriptions for all credit scores.\n",
    "    \"\"\"\n",
    "    conditions = [\n",
    "        (df[target_col] > left_bound) & (df[target_col] <= level_1),\n",
    "        (df[target_col] > level_1) & (df[target_col] <= level_2),\n",
    "        (df[target_col] > level_2) & (df[target_col] <= level_3),\n",
    "        (df[target_col] > level_3) & (df[target_col] <= level_4),\n",
    "        (df[target_col] > level_4) & (df[target_col] <= level_5),\n",
    "        (df[target_col] > level_5) & (df[target_col] <= level_6),\n",
    "        (df[target_col] > level_6) & (df[target_col] <= level_7),\n",
    "        (df[target_col] > level_7) & (df[target_col] <= level_8),\n",
    "        (df[target_col] > level_8) & (df[target_col] <= right_bound),\n",
    "    ]\n",
    "\n",
    "    level_choices = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    lower_bound_choices = [left_bound, level_1, level_2, level_3, level_4, level_5, level_6, level_7, level_8]\n",
    "    upper_bound_choices = [level_1, level_2, level_3, level_4, level_5, level_6, level_7, level_8, right_bound]\n",
    "    df[\"credit_level\"] = np.select(conditions, level_choices)\n",
    "    df[\"credit_lower_bound\"] = np.select(conditions, lower_bound_choices)\n",
    "    df[\"credit_upper_bound\"] = np.select(conditions, upper_bound_choices)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc(y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate ROC AUC (Area Under the Receiver Operating Characteristic Curve).\n",
    "    \n",
    "    Args:\n",
    "        y_true (Union[list, np.array]): True labels.\n",
    "        y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "    Returns:\n",
    "        float: ROC AUC score.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "def pr_auc(y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate PR AUC (Area Under the Precision Recall Curve).\n",
    "    \n",
    "    Args:\n",
    "        y_true (Union[list, np.array]): True labels.\n",
    "        y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "    Returns:\n",
    "        float: PR AUC score.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    return auc(recall, precision)\n",
    "\n",
    "def gini(y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Gini coefficient.\n",
    "\n",
    "    Args:\n",
    "        y_true (Union[list, np.array]): True labels.\n",
    "        y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "    Returns:\n",
    "        float: Gini coefficient.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    return 2 * roc_auc - 1\n",
    "\n",
    "def ks(y_true: Union[list, np.array], y_pred_proba: Union[list, np.array]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Kolmogorov-Smirnov (KS) statistic.\n",
    "\n",
    "    Args:\n",
    "        y_true (Union[list, np.array]): True labels.\n",
    "        y_pred_prob (Union[list, np.array]): Prediction probability of target class of `1`\n",
    "    Returns:\n",
    "        float: KS statistic.\n",
    "    \"\"\"\n",
    "    y_pred_proba_not_default = y_pred_proba[y_true == 0]\n",
    "    y_pred_proba_default = y_pred_proba[y_true == 1]\n",
    "    ks_stat, _ = stats.ks_2samp(y_pred_proba_not_default, y_pred_proba_default)\n",
    "    return ks_stat\n",
    "\n",
    "def plot_calibration_curve(y_true: np.array, y_pred_proba: np.array, model_name: str, figsize: Tuple[int, int], n_bins=10) -> plt.Axes:\n",
    "    \"\"\"\n",
    "    Plot calibration curve.\n",
    "\n",
    "    Args:\n",
    "        y_pred_proba (np.array): Predicted probabilities for the positive class (default).\n",
    "        y_true (np.array): True binary labels (0 for not default, 1 for default).\n",
    "        model_name (str): Name of the model for labeling the plot.\n",
    "        figsize (Tuple[int, int]): size of the plot.\n",
    "        n_bins (int): Number of bins to use for calibration curve.\n",
    "    Return:\n",
    "        plt.Axes: Matplotlib axis object.\n",
    "    \"\"\"\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_pred_proba, n_bins=n_bins)\n",
    "    \n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfectly calibrated\")\n",
    "    ax.plot(prob_pred, prob_true, marker=\"o\", label=model_name)\n",
    "    \n",
    "    ax.set_xlabel(\"Mean predicted probability\")\n",
    "    ax.set_ylabel(\"Fraction of positives\")\n",
    "    ax.set_title(\"Calibration plot\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def print_side_by_side(dict1: dict, dict2: dict) -> None:\n",
    "    \"\"\"\n",
    "    Prints the content of two dictionaries side by side.\n",
    "\n",
    "    Args:\n",
    "        dict1 (dict): The first dictionary to be printed.\n",
    "        dict2 (dict): The second dictionary to be printed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Calculate the maximum length of keys in both dictionaries\n",
    "    max_key_len = max(max(len(key) for key in dict1), max(len(key) for key in dict2))\n",
    "    \n",
    "    # Define the format string for printing, adjusted for floating point numbers\n",
    "    format_str = \"{:<{key_len}}: {:<10} | {:<10}\"\n",
    "    \n",
    "    # Print header\n",
    "    print(format_str.format(\"Metric\", \"Train\", \"Test\", key_len=max_key_len))\n",
    "    \n",
    "    # Print separator\n",
    "    print(\"-\" * (max_key_len + 24))  # Adjusted separator length\n",
    "    \n",
    "    # Print key-value pairs side by side, rounding floats if necessary\n",
    "    for key in dict1:\n",
    "        val1 = dict1[key]\n",
    "        val2 = dict2[key]\n",
    "        \n",
    "        # Check if the values are float, if so, round to 2 decimal places\n",
    "        if isinstance(val1, float):\n",
    "            val1 = round(val1, 2)\n",
    "        if isinstance(val2, float):\n",
    "            val2 = round(val2, 2)\n",
    "        \n",
    "        # Print the formatted output\n",
    "        print(format_str.format(key, val1, val2, key_len=max_key_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_credit_score_distributions(data, features):\n",
    "    \"\"\"\n",
    "    Plot distributions of credit score by a list of categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: DataFrame containing the data.\n",
    "    - features: List of strings representing the categorical features to plot.\n",
    "    \"\"\"\n",
    "    num_features = len(features)\n",
    "    # Calculate the number of rows/columns needed for the subplots\n",
    "    nrows = (num_features + 1) // 2\n",
    "    ncols = 2 if num_features > 1 else 1\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    f, axes = plt.subplots(nrows, ncols, figsize=(18, 6 * nrows))\n",
    "    \n",
    "    # Flatten axes array for easy indexing if there's more than one subplot\n",
    "    if nrows * ncols > 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot distributions of credit score by specified features\n",
    "    for i, feature in enumerate(features):\n",
    "        sns.boxplot(x=feature, y='credit_score', data=data, ax=axes[i])\n",
    "        axes[i].set_title(f'Credit Score by {feature.capitalize()}')\n",
    "        axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
